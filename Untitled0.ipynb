{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMimivirZO3H"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is a parameter?\n",
        "  - A parameter is an internal value of a model that gets learned from the training data.\n",
        "It defines how the model makes predictions.\n",
        "\n",
        "Parameters are not set by you ‚Äî they are automatically adjusted by the learning algorithm.\n",
        "\n",
        "Example: In linear regression, the slope (ùë§) and intercept (b) are parameters.\n",
        "\n",
        "2. Correlation is a statistical measure that tells us how strongly two variables are related to each other and in what direction.\n",
        "\n",
        "It shows whether an increase in one variable is associated with an increase or decrease in another.\n",
        "\n",
        "The correlation coefficient (often called Pearson‚Äôs r) ranges between ‚Äì1 and +1.\n",
        "\n",
        "\n",
        "3. Define Machine Learning. What are the main components in Machine Learning?\n",
        "  -  It is a branch of AI that enables systems to learn from data and make predictions without being explicitly programmed.\n",
        "\n",
        "Main Components of ML\n",
        "\n",
        "Dataset ‚Üí Raw data used for training.\n",
        "\n",
        "Features (X) ‚Üí Input variables.\n",
        "\n",
        "Target (Y) ‚Üí Output to predict.\n",
        "\n",
        "Model / Algorithm ‚Üí Mathematical method that learns from data.\n",
        "\n",
        "Training ‚Üí Process of adjusting model parameters.\n",
        "\n",
        "Parameters ‚Üí Learned values inside the model (e.g., weights).\n",
        "\n",
        "Hyperparameters ‚Üí Settings chosen before training (e.g., learning rate).\n",
        "\n",
        "Evaluation ‚Üí Measures model performance (accuracy, loss).\n",
        "\n",
        "Prediction ‚Üí Using the trained model on new data.\n",
        "\n",
        "4. How does loss value help in determining whether the model is good or not?\n",
        "  - How Loss Helps in Determining Model Quality\n",
        "\n",
        "Training Indicator\n",
        "\n",
        "If the loss is decreasing during training ‚Üí model is learning well.\n",
        "\n",
        "If the loss is stuck or increasing ‚Üí model is not improving.\n",
        "\n",
        "Performance Check\n",
        "\n",
        "A low loss value means the predictions are close to actual outcomes.\n",
        "\n",
        "A high loss value means the model predictions are poor.\n",
        "\n",
        "Avoiding Overfitting / Underfitting\n",
        "\n",
        "Compare training loss vs. validation loss:\n",
        "\n",
        "Both low ‚Üí Good model.\n",
        "\n",
        "Training loss low, validation loss high ‚Üí Overfitting.\n",
        "\n",
        "Both high ‚Üí Underfitting.\n",
        "\n",
        "5. What are continuous and categorical variables?\n",
        "  - Continuous variables are numerical values that can take any value within a given range and are measured rather than counted, such as call duration, age, or satisfaction score. On the other hand, categorical variables represent labels or groups that classify data into distinct categories, such as agent name, department, or whether a call was resolved (Yes/No). In simple terms, continuous variables deal with numbers and measurements, while categorical variables deal with groups and categories.\n",
        "\n",
        "  6. How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "    - Categorical variables cannot be directly fed into most ML models (since they work with numbers). Therefore, we need to convert categories into numerical form using encoding techniques.\n",
        "\n",
        "üîπ Common Techniques\n",
        "\n",
        "Label Encoding\n",
        "\n",
        "Assigns a unique number to each category.\n",
        "\n",
        "Example: Department ‚Üí {Sales=0, Support=1, Billing=2}.\n",
        "\n",
        "Simple but may imply an unwanted order.\n",
        "\n",
        "One-Hot Encoding\n",
        "\n",
        "Creates a new binary column (0/1) for each category.\n",
        "\n",
        "Example: Department ‚Üí {Sales=[1,0,0], Support=[0,1,0], Billing=[0,0,1]}.\n",
        "\n",
        "Most commonly used technique.\n",
        "\n",
        "Ordinal Encoding\n",
        "\n",
        "Used when categories have a natural order (e.g., Low=1, Medium=2, High=3).\n",
        "\n",
        "Works well for ordered variables but not for unordered labels.\n",
        "\n",
        "Target Encoding (Mean Encoding)\n",
        "\n",
        "Replaces categories with the mean of the target variable for that category.\n",
        "\n",
        "Useful for high-cardinality variables but must be applied carefully to avoid data leakage.\n",
        "\n",
        "Frequency Encoding\n",
        "\n",
        "Categories are replaced with their frequency/count in the dataset.\n",
        "\n",
        "Example: If \"Billing\" appears 200 times, \"Support\" 100 times ‚Üí values become {200, 100}.\n",
        "\n",
        "7.What do you mean by training and testing a dataset?\n",
        " - Training and testing a dataset refers to splitting the available data into two parts to evaluate the performance of a machine learning model. The training dataset is the portion of data used to teach the model patterns, relationships, and rules by adjusting its parameters. Once the model is trained, it is evaluated on the testing dataset, which is new and unseen by the model during training. This helps check whether the model generalizes well to new data instead of just memorizing the training examples. In short, training data is for learning, and testing data is for measuring performance.\n",
        "\n",
        " 8.What is sklearn.preprocessing?\n",
        "  - sklearn.preprocessing is a module in Scikit-learn (sklearn) that provides tools for preprocessing data before feeding it into a machine learning model. Preprocessing is important because raw data often has different scales, formats, or types that can negatively impact model performance. This module includes methods for scaling (e.g., StandardScaler, MinMaxScaler), normalization, encoding categorical variables (LabelEncoder, OneHotEncoder), imputing missing values, and generating polynomial features. In short, sklearn.preprocessing helps transform raw data into a clean, numerical, and standardized format suitable for machine learning algorithms.\n",
        "\n",
        "   9. What is a Test set?\n",
        "    - A test set is a portion of the dataset that is kept aside and not used during model training. After the model is trained on the training set, the test set is used to evaluate how well the model performs on unseen data. It helps check whether the model can generalize to new situations instead of just memorizing the training data. In short, the test set acts like a final exam for the model ‚Äî it shows the true accuracy, error, or performance on data it has never seen before.\n",
        "     \n",
        "     10. How do we split data for model fitting (training and testing) in Python?\n",
        "How do you approach a Machine Learning problem?\n",
        "  - from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Suppose X = features, y = target\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 80% data ‚Üí training, 20% data ‚Üí testing\n",
        "\n",
        "11. Why do we have to perform EDA before fitting a model to the data?\n",
        "  - We perform Exploratory Data Analysis (EDA) before fitting a model because it helps us understand the data and ensures the model is built on clean, meaningful inputs. EDA allows us to detect missing values, duplicates, and outliers, identify the distribution of variables, and check relationships or correlations between features and the target variable. It also helps in distinguishing between categorical and continuous variables, guiding which preprocessing or encoding techniques to apply. Without EDA, the model may be trained on biased, noisy, or incomplete data, leading to poor performance and misleading results. In short, EDA is like diagnosing the data‚Äôs health before applying ML ‚Äî it ensures we fit the right model on the right data.\n",
        "\n",
        "  12. same above\n",
        "  13. sAME ABOVE\n",
        "\n",
        "  14.How can you find correlation between variables in Python?\n",
        "   - Usind pandas corr()\n",
        "      import pandas as pd\n",
        "\n",
        "# Example DataFrame\n",
        "data = {\n",
        "    \"Call_Duration\": [5, 7, 10, 4, 9],\n",
        "    \"Customer_Satisfaction\": [3, 4, 5, 2, 5]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Correlation matrix\n",
        "print(df.corr())\n",
        "\n",
        "15.What is causation? Explain difference between correlation and causation with an example.\n",
        " -  Causation means that one variable directly affects or brings about a change in another variable. In other words, there is a cause-and-effect relationship.\n",
        "\n",
        "üîπ Correlation vs Causation\n",
        "\n",
        "Correlation ‚Üí Two variables move together (positively or negatively), but one does not necessarily cause the other.\n",
        "\n",
        "Causation ‚Üí One variable‚Äôs change is the reason for the change in another variable.\n",
        "\n",
        "üîπ Example\n",
        "\n",
        "Correlation Example:\n",
        "Ice cream sales and drowning cases both increase in summer.\n",
        " They are correlated because of the season (temperature), but buying ice cream does not cause drowning.\n",
        "\n",
        "Causation Example:\n",
        "More hours of study ‚Üí higher exam scores.\n",
        " Here, studying causes improvement in performance.\n",
        "   \n",
        "   16. What is an Optimizer? What are different types of optimizers? Explain each with an example\n",
        "    - An optimizer in machine learning is an algorithm that updates the parameters of a model, such as weights and biases, to minimize the loss function and improve accuracy. The simplest form is Gradient Descent, which updates parameters in the direction of the steepest descent of the loss. Stochastic Gradient Descent (SGD) improves speed by updating parameters using one sample or small batches at a time, while Mini-batch Gradient Descent balances efficiency and stability. Advanced optimizers like Momentum help accelerate learning, Adagrad adjusts learning rates for sparse data, RMSProp stabilizes learning by using moving averages of gradients, and Adam, the most widely used, combines the benefits of Momentum and RMSProp to adaptively adjust learning rates for efficient and stable training. In short, optimizers guide the model toward the best solution by fine-tuning its parameters.\n",
        "\n",
        "    17. What is sklearn.linear_model ?\n",
        "      - sklearn.linear_model is a module in scikit-learn (a popular Python library for machine learning) that provides linear models for regression and classification tasks. These models assume a linear relationship between input features (independent variables) and the target variable (dependent variable)\n",
        "\n",
        "      18. What does model.fit() do? What arguments must be given?\n",
        "     - model.fit(X, y)\n",
        "       The model learns the relationship between the features (X) and the target (y).\n",
        "\n",
        "Internally, it calculates parameters like:\n",
        "\n",
        "In linear regression: coefficients (w1, w2, ...) and intercept (b)\n",
        "\n",
        "In logistic regression: weights for classification\n",
        "\n",
        "In tree-based models: splits, thresholds, etc.\n",
        "\n",
        "After fitting, the model can be used to predict new data using model.predict().\n",
        "Arguments of fit()\n",
        "\n",
        "The arguments depend on the type of model, but generally:\n",
        "\n",
        "For supervised learning (regression/classification):\n",
        "model.fit(X, y, sample_weight=None)\n",
        "\n",
        "\n",
        "X: Input features (2D array-like: [n_samples, n_features])\n",
        "\n",
        "Example: NumPy array, pandas DataFrame\n",
        "\n",
        "y: Target variable (1D array for regression or classification)\n",
        "\n",
        "Example: [2, 4, 6, 8] or [0, 1, 0, 1]\n",
        "\n",
        "sample_weight (optional): Give weights to samples if some points are more important.\n",
        "\n",
        "19. What does model.predict() do? What arguments must be given?\n",
        "  - It takes new input data and applies the model‚Äôs learned parameters to generate predictions.\n",
        "\n",
        "Essentially, it computes the output based on what the model learned during training.\n",
        "\n",
        "Examples:\n",
        "\n",
        "Linear regression: Uses coefficients and intercept to calculate y = w1*x1 + w2*x2 + ... + b.\n",
        "\n",
        "Logistic regression: Computes probability and assigns class labels (0 or 1).\n",
        "\n",
        "Tree-based models: Follows splits in the tree to assign a predicted value or class.\n",
        "\n",
        "2Ô∏è‚É£ Arguments\n",
        "model.predict(X)\n",
        "\n",
        "\n",
        "X: Input features for which you want predictions.\n",
        "\n",
        "Must have same number of features as used during fit().\n",
        "\n",
        "Shape: [n_samples, n_features]\n",
        "\n",
        "‚ö†Ô∏è You do not provide the target variable y here. predict() is only for new/unseen data.\n",
        "\n",
        "What are continuous and categorical variables?\n",
        " - Continuous and categorical variables are two fundamental types of variables used in data analysis and machine learning. Continuous variables are numeric and can take an infinite number of values within a range; they are measurable and allow arithmetic operations such as addition, subtraction, or averaging. Examples include height, weight, temperature, age, or salary, where values can be decimals and vary continuously. On the other hand, categorical variables represent distinct groups or categories and are qualitative in nature. They cannot be meaningfully added or averaged. Categorical variables are further divided into nominal, which have no inherent order, like gender or product type, and ordinal, which have a natural order, like education level or rating scales. In practice, continuous variables are often used directly in models, while categorical variables are usually encoded into numeric forms before being used in machine learning algorithms.\n",
        "\n",
        "   21. What is feature scaling? How does it help in Machine Learning?\n",
        "     - Feature scaling is a data preprocessing technique used in machine learning to standardize or normalize the range of independent variables (features) so that they are on a comparable scale. Different features in a dataset can have widely different ranges‚Äîfor example, one feature might range from 0 to 1 (like a probability), while another ranges from 1,000 to 100,000 (like income). If features are on different scales, many machine learning algorithms may perform poorly or take longer to converge.\n",
        "\n",
        "How Feature Scaling Helps\n",
        "\n",
        "Faster Convergence in Gradient-Based Algorithms\n",
        "\n",
        "Algorithms like linear regression, logistic regression, or neural networks use gradient descent.\n",
        "\n",
        "If features have very different scales, the optimization process may take a long zig-zag path to reach the minimum, slowing convergence.\n",
        "\n",
        "Prevents Bias Toward Large-Scale Features\n",
        "\n",
        "Algorithms that compute distances or similarities, like K-Nearest Neighbors (KNN), K-Means, or Support Vector Machines (SVM), are sensitive to feature magnitudes.\n",
        "\n",
        "Without scaling, a feature with a larger range can dominate the distance calculation, making the model biased.\n",
        "\n",
        "Improves Model Accuracy\n",
        "\n",
        "Properly scaled features help the model learn relationships more effectively, often improving performance.\n",
        "\n",
        " 22. How do we perform scaling in Python?\n",
        "   - In Python, feature scaling is commonly performed using scikit-learn‚Äôs preprocessing module. The two main techniques are Standardization and Normalization (Min-Max Scaling). Here‚Äôs a detailed explanation with examples:\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Sample data\n",
        "X = np.array([[1000, 0.5],\n",
        "              [2000, 0.75],\n",
        "              [3000, 0.25]])\n",
        "\n",
        "# Initialize scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the data\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(\"Standardized Data:\\n\", X_scaled)\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Initialize scaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit and transform the data\n",
        "X_norm = scaler.fit_transform(X)\n",
        "\n",
        "print(\"Normalized Data:\\n\", X_norm)\n",
        "\n",
        "\n",
        "23. What is sklearn.preprocessing?\n",
        "  - sklearn.preprocessing is a module in scikit-learn that provides tools for preparing and transforming data before feeding it into machine learning models. Preprocessing is a crucial step because most ML algorithms perform better when the input data is properly scaled, normalized, or encoded\n",
        "\n",
        "  24. Using train_test_split\n",
        "\n",
        "Import the function:\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "Basic Syntax:\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, shuffle=True\n",
        ")\n",
        "\n",
        "\n",
        "Parameters:\n",
        "\n",
        "Parameter\tDescription\n",
        "X\tFeature matrix (independent variables)\n",
        "y\tTarget vector (dependent variable)\n",
        "test_size\tFraction of data to use as test set (e.g., 0.2 = 20%)\n",
        "train_size\tFraction of data for training (optional if test_size is given)\n",
        "random_state\tSeed for reproducibility of the split\n",
        "shuffle\tWhether to shuffle data before splitting (default: True)\n",
        "3Ô∏è‚É£ Example\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Sample data\n",
        "X = np.array([[1], [2], [3], [4], [5]])\n",
        "y = np.array([2, 4, 6, 8, 10])\n",
        "\n",
        "# Split into 80% training and 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"X_train:\", X_train)\n",
        "print(\"X_test:\", X_test)\n",
        "print(\"y_train:\", y_train)\n",
        "print(\"y_test:\", y_test)\n",
        "\n",
        "\n",
        "Output:\n",
        "\n",
        "X_train: [[5]\n",
        "          [3]\n",
        "          [1]\n",
        "          [4]]\n",
        "X_test: [[2]]\n",
        "y_train: [10  6  2  8]\n",
        "y_test: [4]\n",
        "\n",
        "4Ô∏è‚É£ Key Notes\n",
        "\n",
        "Typical split ratios: 70/30, 80/20, or 75/25.\n",
        "\n",
        "Always split before scaling or preprocessing to avoid data leakage.\n",
        "\n",
        "For classification problems, use stratified splitting to maintain class proportions:\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "25.Explain data encoding?\n",
        "  - Data encoding is the process of converting categorical or textual data into numeric form so that machine learning models can process it. Most machine learning algorithms require numeric input, so categorical data (like ‚ÄúRed‚Äù, ‚ÄúBlue‚Äù, ‚ÄúGreen‚Äù or ‚ÄúMale‚Äù, ‚ÄúFemale‚Äù) must be transformed into numbers."
      ],
      "metadata": {
        "id": "XBbcNUy3ZXQh"
      }
    }
  ]
}